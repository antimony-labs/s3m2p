//! â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//! FILE: lessons.rs | src/lessons.rs
//! PURPOSE: Lesson definitions and curriculum structure
//! MODIFIED: 2025-11-29
//! LAYER: LEARN â†’ src
//! â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// Lesson definitions - ML curriculum

/// A single lesson
pub struct Lesson {
    pub id: usize,
    pub title: &'static str,
    pub subtitle: &'static str,
    pub phase: &'static str,
    pub icon: &'static str,
    pub description: &'static str,
    pub intuition: &'static str,
    pub math: &'static str, // KaTeX formatted
    pub key_concepts: &'static [&'static str],
}

/// All lessons in the curriculum
pub static LESSONS: &[Lesson] = &[
    // Phase 1: Foundations
    Lesson {
        id: 0,
        title: "Rust Refresher",
        subtitle: "Ownership & Borrowing",
        phase: "Foundations",
        icon: "ğŸ¦€",
        description: "Memory safety without garbage collection. Understanding ownership, borrowing, and lifetimes.",
        intuition: "Imagine every piece of data has exactly one owner. When you want to share, you can either lend it temporarily (borrow) or give it away permanently (move).",
        math: "",
        key_concepts: &["Ownership", "Borrowing", "Lifetimes", "Move semantics"],
    },
    Lesson {
        id: 1,
        title: "Linear Regression",
        subtitle: "Fitting a Line",
        phase: "Foundations",
        icon: "ğŸ“ˆ",
        description: "The simplest ML model. Find the best line through data points using gradient descent.",
        intuition: "Draw a line through scattered points. Adjust the line's slope and position until the total distance from all points to the line is minimized.",
        math: r"y = wx + b \quad \text{Loss} = \frac{1}{n}\sum(y - \hat{y})^2",
        key_concepts: &["Gradient Descent", "Loss Function", "Learning Rate", "MSE"],
    },
    Lesson {
        id: 2,
        title: "Logistic Regression",
        subtitle: "Classification",
        phase: "Foundations",
        icon: "ğŸ¯",
        description: "Binary classification with decision boundaries. Predict probabilities, not just values.",
        intuition: "Instead of a line, we curve it with a sigmoid function. Output becomes a probability between 0 and 1.",
        math: r"\sigma(z) = \frac{1}{1 + e^{-z}} \quad P(y=1|x) = \sigma(wx + b)",
        key_concepts: &["Sigmoid", "Cross-Entropy", "Decision Boundary", "Probability"],
    },
    Lesson {
        id: 3,
        title: "Neural Networks",
        subtitle: "Universal Approximators",
        phase: "Foundations",
        icon: "ğŸ§ ",
        description: "Layers of neurons that can learn any function. Backpropagation and automatic differentiation.",
        intuition: "Stack multiple linear transformations with non-linear activations. Each layer extracts higher-level features.",
        math: r"h = \text{ReLU}(W_1 x + b_1) \quad y = W_2 h + b_2",
        key_concepts: &["Backpropagation", "Activation Functions", "Hidden Layers", "Autograd"],
    },

    // Phase 2: Deep Learning
    Lesson {
        id: 4,
        title: "CNNs",
        subtitle: "Convolution & Vision",
        phase: "Deep Learning",
        icon: "ğŸ‘ï¸",
        description: "Convolutional neural networks for image processing. Filters, pooling, and feature maps.",
        intuition: "Slide a small filter across an image. Each position produces one output value. Stack many filters to detect edges, textures, shapes.",
        math: r"(f * g)[n] = \sum_{m} f[m] \cdot g[n-m]",
        key_concepts: &["Convolution", "Pooling", "Feature Maps", "Receptive Field"],
    },
    Lesson {
        id: 5,
        title: "Policy Networks",
        subtitle: "State â†’ Action",
        phase: "Deep Learning",
        icon: "ğŸ®",
        description: "Neural networks that output action probabilities. The foundation of deep reinforcement learning.",
        intuition: "Given a game state, output probabilities for each possible action. Train by reinforcing actions that led to wins.",
        math: r"\pi(a|s) = \text{softmax}(f_\theta(s))",
        key_concepts: &["Policy", "Softmax", "Action Space", "State Representation"],
    },

    // Phase 3: Reinforcement Learning
    Lesson {
        id: 6,
        title: "Q-Learning",
        subtitle: "Value-Based RL",
        phase: "Reinforcement Learning",
        icon: "ğŸ—ºï¸",
        description: "Learn action values through exploration. The Q-table and Îµ-greedy exploration.",
        intuition: "Build a table of how good each action is in each state. Update estimates based on rewards received.",
        math: r"Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]",
        key_concepts: &["Q-Values", "Bellman Equation", "Îµ-Greedy", "Temporal Difference"],
    },
    Lesson {
        id: 7,
        title: "Policy Gradients",
        subtitle: "Actor-Critic",
        phase: "Reinforcement Learning",
        icon: "ğŸ­",
        description: "Directly optimize the policy. Actor proposes actions, critic evaluates them.",
        intuition: "Actor tries actions, critic scores them. Actor improves by doing more of what critic likes.",
        math: r"\nabla J(\theta) = \mathbb{E}[\nabla \log \pi(a|s) \cdot A(s,a)]",
        key_concepts: &["Policy Gradient", "Advantage", "Actor-Critic", "REINFORCE"],
    },
    Lesson {
        id: 8,
        title: "MCTS",
        subtitle: "Monte Carlo Tree Search",
        phase: "Reinforcement Learning",
        icon: "ğŸŒ³",
        description: "Look-ahead planning through simulation. Selection, expansion, simulation, backpropagation.",
        intuition: "Build a tree of possible futures. Simulate random games from each position. Pick moves that lead to wins most often.",
        math: r"UCB1 = \bar{x}_i + c\sqrt{\frac{\ln N}{n_i}}",
        key_concepts: &["Tree Search", "UCB", "Rollouts", "Exploration vs Exploitation"],
    },

    // Phase 4: Advanced
    Lesson {
        id: 9,
        title: "AlphaZero",
        subtitle: "Self-Play Mastery",
        phase: "Advanced",
        icon: "â™Ÿï¸",
        description: "Combine neural networks with MCTS. Learn entirely from self-play, no human knowledge.",
        intuition: "Play against yourself millions of times. Use a neural net to evaluate positions and guide search.",
        math: r"p, v = f_\theta(s) \quad \text{MCTS guided by } (p, v)",
        key_concepts: &["Self-Play", "Neural MCTS", "Policy + Value Head", "Zero Knowledge"],
    },
    Lesson {
        id: 10,
        title: "LLMs",
        subtitle: "Transformers & Attention",
        phase: "Advanced",
        icon: "ğŸ’¬",
        description: "Large language models and the transformer architecture. Attention is all you need.",
        intuition: "Each word attends to every other word. Relevance determined by learned queries and keys.",
        math: r"\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V",
        key_concepts: &["Attention", "Transformer", "Tokenization", "Autoregressive"],
    },
    Lesson {
        id: 11,
        title: "System Architecture",
        subtitle: "Putting It Together",
        phase: "Advanced",
        icon: "ğŸŒŒ",
        description: "Multimodal processing, memory systems, reasoning engines. Building integrated AI systems.",
        intuition: "Combine perception (vision, language), memory (short/long term), and reasoning (planning, inference) into one system.",
        math: "",
        key_concepts: &["Multimodal", "Memory", "Reasoning", "World Models"],
    },
];
